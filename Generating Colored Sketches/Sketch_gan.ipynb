{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketchy_gan_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W18W7a2oGb9S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#Discriminator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64,128,256, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels*2, features[0], kernel_size=4,stride=1, padding=\"same\", padding_mode=\"reflect\",),\n",
        "        nn.LeakyReLU(0.2),# 256 x 256 x 64\n",
        "        nn.Conv2d(features[0], features[1], kernel_size=4,stride=1, padding=\"same\", padding_mode=\"reflect\",),\n",
        "        nn.BatchNorm2d(features[1]),\n",
        "        nn.LeakyReLU(0.2),# 256 x 256 x 128\n",
        "        nn.MaxPool2d(kernel_size=4,stride=2),#127 x 127 x128\n",
        "        nn.Conv2d(features[1], features[2], kernel_size=4,stride=1, padding=\"same\", padding_mode=\"reflect\",),\n",
        "        nn.BatchNorm2d(features[2]),\n",
        "        nn.LeakyReLU(0.2),#127 x 127 x256\n",
        "        nn.Conv2d(features[2], features[3], kernel_size=4, stride=1,padding=\"valid\", padding_mode=\"reflect\",),\n",
        "        nn.BatchNorm2d(features[3]),\n",
        "        nn.LeakyReLU(0.2),# 124 x 124 x 256\n",
        "        nn.MaxPool2d(kernel_size=4,stride=2), #61 x 61 x 256\n",
        "        nn.Conv2d(features[3], features[4], kernel_size=4,stride =1, padding=\"valid\", padding_mode=\"reflect\",),\n",
        "        nn.BatchNorm2d(features[4]),\n",
        "        nn.LeakyReLU(0.2), # 58 x 58 x 512\n",
        "        nn.MaxPool2d(kernel_size=4,stride=2), # 28 x 28 x 512\n",
        "        nn.Conv2d(features[4], 1, kernel_size=4,stride=1, padding= \"same\", padding_mode=\"reflect\",),# 28 x 28 x 1\n",
        "    )\n",
        "    \n",
        "\n",
        "  def forward(self, x, y):\n",
        "    x = torch.cat([x,y],1)# concatenating input image and generated image\n",
        "    x = self.model(x)# sending input to initial layer\n",
        "    # print(x.shape)\n",
        "    return x\n",
        "    # x = torch.cat([x,y],1) # concatenating input image and generated image\n",
        "    # x = self.initial(x)\n",
        "    # return self.model(x) \n",
        "    \n",
        "\n",
        "\n",
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  y = torch.randn((1, 3, 256, 256))\n",
        "  model = Discriminator()\n",
        "  preds = model(x,y)\n",
        "  print(preds.shape)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTvS9UzRXwYy"
      },
      "source": [
        "#generator\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 4 ,2,1, bias=False, padding_mode=\"reflect\")\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels, out_channels, 4,2,1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2)\n",
        "\n",
        "    )\n",
        "    self.use_dropout = use_dropout\n",
        "    self.dropout= nn.Dropout(0.5) \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=64):\n",
        "    super().__init__()\n",
        "    self.initial_down = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, features, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.LeakyReLU(0.2),# 256 x 256 x 64\n",
        "        nn.Conv2d(features, features, 4, 2, 1, padding_mode = \"reflect\"),\n",
        "        nn.LeakyReLU(0.2), # 128 x 128 x 64\n",
        "    )#initial layer\n",
        "    \n",
        "    self.l1 = nn.Sequential(\n",
        "        nn.Conv2d(features, features*2, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    )# 128 x 128 x 128\n",
        "    self.down1 = Block(features*2, features*2, down = True, act=\"leaky\", use_dropout=False)# next layer 64 x 64 x 128 \n",
        "\n",
        "    self.l2 = nn.Sequential(\n",
        "        nn.Conv2d(features*2, features*4, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    )# 64 x 64 x 256\n",
        "    self.down2 = Block(features*4, features*4, down = True, act=\"leaky\", use_dropout=False)# next layer 32 x 32 x 256\n",
        "\n",
        "    self.l3 = nn.Sequential(\n",
        "        nn.Conv2d(features*4, features*8, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    )# 32 x 32 x 512\n",
        "    self.down3 = Block(features*8, features*8, down = True, act=\"leaky\", use_dropout=False)# next layer 16 x 16 x 512\n",
        "\n",
        "    self.l4 = nn.Sequential(\n",
        "        nn.Conv2d(features*8, features*8, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    )# 16 x 16 x 512\n",
        "    self.down4 = Block(features*8, features*8, down = True, act=\"leaky\", use_dropout=False)# next layer 8 x 8 x 512\n",
        "\n",
        "    self.l5 = nn.Sequential(\n",
        "        nn.Conv2d(features *8, features*8, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    )# 8 x 8 x 512\n",
        "    self.down5 = Block(features*8, features*8, down = True, act=\"leaky\", use_dropout=False)# next layer 4 x 4 x 512\n",
        "\n",
        "    self.l6 = nn.Sequential(\n",
        "        nn.Conv2d(features*8, features*8, 3, 1, padding=\"same\", padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(features*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    \n",
        "    ) # 4 x 4 x 512\n",
        "    self.down6 = Block(features*8, features*8, down = True, act=\"leaky\", use_dropout=False)# next layer 2 x 2 x 512\n",
        "\n",
        "\n",
        "    self.bottleneck = nn.Sequential(\n",
        "        nn.Conv2d(features*8, features*16, 4, 2, 1, padding_mode=\"reflect\"), \n",
        "        nn.ReLU()\n",
        "\n",
        "    )#feature space 1 x 1 x 1024\n",
        "\n",
        "    self.up1 = Block(features*16, features*8, down=False, act=\"relu\", use_dropout=True) #upsampling\n",
        "    self.up2 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)#upsampling\n",
        "    self.up3 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)#upsampling\n",
        "    self.up4 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=False)#upsampling\n",
        "    self.up5 = Block(features*8*2, features*4, down=False, act=\"relu\", use_dropout=False)#upsampling\n",
        "    self.up6 = Block(features*4*2, features*2, down=False, act=\"relu\", use_dropout=False)#upsampling\n",
        "    self.up7 = Block(features*2*2, features, down=False, act=\"relu\", use_dropout=False)#upsampling\n",
        "\n",
        "    self.final_up = nn.Sequential(\n",
        "        nn.ConvTranspose2d(features*2,in_channels, kernel_size=4,stride=2, padding=1),\n",
        "        nn.Tanh(),\n",
        "    )# final layer (-1,3,256,256)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    d1 = self.initial_down(x)\n",
        "    # print(\"d1\", d1.shape)\n",
        "\n",
        "    l1 = self.l1(d1)\n",
        "    # print(\"l1\", l1.shape)\n",
        "    d2 = self.down1(l1)\n",
        "    # print(\"d2\", d2.shape)\n",
        "\n",
        "    l2 = self.l2(d2)\n",
        "    # print(\"l2\", l2.shape)\n",
        "    d3 = self.down2(l2)\n",
        "    # print(\"d3\", d3.shape)\n",
        "\n",
        "    l3 = self.l3(d3)\n",
        "    # print(\"l3\", l3.shape)\n",
        "    d4 = self.down3(l3)\n",
        "    # print(\"d4\", d4.shape)\n",
        "\n",
        "    l4 = self.l4(d4)\n",
        "    # print(\"l4\", l4.shape)\n",
        "    d5 = self.down4(d4)\n",
        "    # print(\"d5\", d5.shape)\n",
        "\n",
        "    l5 = self.l5(d5)\n",
        "    # print(\"l5\", l5.shape)\n",
        "    d6 = self.down5(l5)\n",
        "    # print(\"d6\", d6.shape)\n",
        "\n",
        "    l6 = self.l6(d6)\n",
        "    # print(\"l6\", l6.shape)\n",
        "    d7 = self.down6(l6)\n",
        "    # print(\"d7\", d7.shape)\n",
        "\n",
        "    bottleneck = self.bottleneck(d7)\n",
        "\n",
        "\n",
        "    up1 = self.up1(bottleneck)\n",
        "    up2 = self.up2(torch.cat([up1, d7],1))# skip connections\n",
        "    up3 = self.up3(torch.cat([up2, d6],1))\n",
        "    up4 = self.up4(torch.cat([up3, d5],1))\n",
        "    up5 = self.up5(torch.cat([up4, d4],1))\n",
        "    up6 = self.up6(torch.cat([up5, d3],1))\n",
        "    up7 = self.up7(torch.cat([up6, d2],1))\n",
        "\n",
        "    return self.final_up(torch.cat([up7, d1], 1))\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXY68q6gkquD"
      },
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEnqP0u5ph5W"
      },
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "#Hyper perameters\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAIN_DIR = \"drive/MyDrive/Sketch_gan/file_name\"# Enter your own path for train data\n",
        "VAL_DIR = \"drive/MyDrive/Sketch_gan/file_name\"# Enter your own path for val data\n",
        "TEST_DIR = \"drive/MyDrive/Sketch_gan/file_name\"# Enter your own path for test data\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS_IMG = 3\n",
        "L1_LAMBDA = 1000\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "CHECKPOINT_DISC = \"drive/MyDrive/Sketch_gan/file_name\"# Enter your own path for saving and loading weights\n",
        "CHECKPOINT_GEN = \"drive/MyDrive/Sketch_gan/file_name\"# Enter your own path for saving and loading weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUASZCoTGmE2"
      },
      "source": [
        "from torchvision.utils import save_image\n",
        "# saving weights and examples\n",
        "def save_some_examples(gen, loader, epoch, folder):\n",
        "\n",
        "    x, y = next(iter(val_loader))\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        y_fake = gen(x)\n",
        "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
        "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
        "        \n",
        "    gen.train()\n",
        "\n",
        "def save_some_test_examples(gen, test_loader, epoch, folder):\n",
        "    x = next(iter(test_loader))\n",
        "    x = x.to(DEVICE)\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        y_fake = gen(x)\n",
        "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
        "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
        "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
        "       \n",
        "    gen.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvrWrkgFIok-"
      },
      "source": [
        "import numpy as np\n",
        "# import config\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import cv2 as cv\n",
        "\n",
        "#loading dataset\n",
        "class SketchDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.x_root_dir = root_dir + \"/x_label\"\n",
        "        self.x_list_files = os.listdir(self.x_root_dir)\n",
        "        self.y_root_dir = root_dir + \"/y_label\"\n",
        "        self.y_list_files = os.listdir(self.y_root_dir)\n",
        "        self.x_sorted_files =  sorted(self.x_list_files)\n",
        "        self.y_sorted_files =  sorted(self.y_list_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_list_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_img_file = self.x_sorted_files[index]\n",
        "        x_img_path = os.path.join(self.x_root_dir, x_img_file)\n",
        "        y_img_file = self.y_sorted_files[index]\n",
        "        y_img_path = os.path.join(self.y_root_dir, y_img_file)\n",
        "\n",
        "        input_image = np.array(Image.open(x_img_path).convert('RGB'))\n",
        "        target_image = np.array(Image.open(y_img_path).convert('RGB'))\n",
        "        \n",
        "        \n",
        "        both_transform = A.Compose(\n",
        "        [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n",
        "        )\n",
        "\n",
        "        transform_only_input = A.Compose(\n",
        "            [        \n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "        ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        transform_only_mask = A.Compose(\n",
        "            [\n",
        "                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        augmentations = both_transform(image=input_image, image0=target_image)\n",
        "        input_image = augmentations[\"image\"]\n",
        "        target_image = augmentations[\"image0\"]\n",
        "        \n",
        "\n",
        "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
        "        target_image = transform_only_mask(image=target_image)[\"image\"]\n",
        "\n",
        "        \n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NFgaa996j84"
      },
      "source": [
        "class TestSketchDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.x_root_dir = root_dir + \"/x_label\"\n",
        "        self.x_list_files = os.listdir(self.x_root_dir)\n",
        "       \n",
        "        self.x_sorted_files =  sorted(self.x_list_files)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_list_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_img_file = self.x_sorted_files[index]\n",
        "        x_img_path = os.path.join(self.x_root_dir, x_img_file)\n",
        "        \n",
        "\n",
        "        input_image = np.array(Image.open(x_img_path).convert('RGB'))\n",
        "        \n",
        "        \n",
        "        \n",
        "        transform = A.Compose(\n",
        "        [A.Resize(width=256, height=256),]\n",
        "        )\n",
        "\n",
        "        transform_only_input = A.Compose(\n",
        "            [        \n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "        ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        augmentations = transform(image=input_image)\n",
        "        input_image = augmentations[\"image\"]\n",
        "        \n",
        "    \n",
        "      \n",
        "\n",
        "        input_image = transform_only_input(image=input_image)[\"image\"]\n",
        "        \n",
        "        \n",
        "        return input_image\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGB40V45mq0u"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf9wmt0gTF3g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40AI8T4tan_s"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "#training model\n",
        "\n",
        "def train_fn(\n",
        "    disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        # Train Discriminator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_fake = gen(x)\n",
        "            D_real = disc(x, y)\n",
        "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
        "            D_fake = disc(x, y_fake.detach())\n",
        "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
        "            D_loss = (D_real_loss + D_fake_loss) / 32\n",
        "\n",
        "        disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            D_fake = disc(x, y_fake)\n",
        "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
        "            G_loss = G_fake_loss + L1\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            loop.set_postfix(\n",
        "                D_real=torch.sigmoid(D_real).mean().item(),\n",
        "                D_fake=torch.sigmoid(D_fake).mean().item(),\n",
        "            )\n",
        "\n",
        "\n",
        "def main():\n",
        "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "    gen = Generator(in_channels=3, features=64).to(DEVICE)\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    BCE = nn.BCEWithLogitsLoss()\n",
        "    L1_LOSS = nn.L1Loss()\n",
        "    \n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_epoch = 21 # enter the epoch number of weights to load\n",
        "        gen_file = f\"/gen_{load_epoch}.pth.tar\"\n",
        "        disc_file = f\"/disc_{load_epoch}.pth.tar\"\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN+gen_file, gen, opt_gen, LEARNING_RATE,\n",
        "        )\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_DISC+disc_file, disc, opt_disc, LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "    train_dataset = SketchDataset(root_dir=TRAIN_DIR)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "    )\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "    val_dataset = SketchDataset(root_dir=VAL_DIR)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
        "    test_dataset = TestSketchDataset(root_dir=TEST_DIR)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "    save_some_examples(gen, val_loader, 1, folder=\"drive/MyDrive/Sketch_gan/val_eval_simha\")\n",
        "    save_some_test_examples(gen, test_loader, 1, folder=\"drive/MyDrive/Sketch_gan/test_eval_simha\")\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(\n",
        "            disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler,\n",
        "        )\n",
        "\n",
        "        if SAVE_MODEL and epoch % 1 == 0:\n",
        "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN+ f\"/gen_{epoch}.pth.tar\")\n",
        "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC+ f\"/disc_{epoch}.pth.tar\")\n",
        "\n",
        "        save_some_examples(gen, val_loader, 22+epoch, folder=\"drive/MyDrive/Sketch_gan/val_eval_simha\")\n",
        "        # save_some_test_examples(gen, test_loader, epoch, folder=\"drive/MyDrive/Sketch_gan/test_eval_simha\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3bqUUdS3eqM"
      },
      "source": [
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG14JIvU1oZq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAodJIrQ47Dy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}